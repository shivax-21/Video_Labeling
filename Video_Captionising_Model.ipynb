{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZLyFTFLSyeV",
        "outputId": "7e6ce00f-b57f-44ec-ba36-0b3d9e76c4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision opencv-python Pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from PIL import Image # Pillow for image processing\n",
        "\n",
        "def setup_captioning_pipeline(model_name=\"Salesforce/blip-image-captioning-base\"):\n",
        "    \"\"\"\n",
        "    Sets up the Hugging Face image-to-text pipeline (VLM).\n",
        "    You can try \"Salesforce/blip-image-captioning-large\" for potentially better results,\n",
        "    but it will require more computational resources.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the image-to-text pipeline with the chosen VLM model\n",
        "        # The 'device' parameter can be set to 0 for GPU, or -1 for CPU.\n",
        "        # If you have a GPU, set device=0 for much faster processing.\n",
        "        # If you don't have a GPU or encounter CUDA errors, use device=-1.\n",
        "        caption_pipeline = pipeline(\"image-to-text\", model=model_name, device=0)\n",
        "        print(f\"Successfully loaded VLM model: {model_name}\")\n",
        "        return caption_pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_name}: {e}\")\n",
        "        print(\"Falling back to CPU if GPU failed. Try setting device=-1 if issues persist.\")\n",
        "        try:\n",
        "            caption_pipeline = pipeline(\"image-to-text\", model=model_name, device=-1)\n",
        "            return caption_pipeline\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load model even on CPU: {e}\")\n",
        "            return None\n",
        "\n",
        "def process_video_for_captions(video_path, caption_pipeline, frame_interval=30):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video at a specified interval and generates captions for them.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the input video file.\n",
        "        caption_pipeline: The Hugging Face image-to-text pipeline.\n",
        "        frame_interval (int): How often to sample frames (e.g., 30 means every 30th frame).\n",
        "                              A higher number means fewer frames, faster processing, but less detail.\n",
        "                              A lower number means more frames, slower processing, but more detail.\n",
        "                              For typical videos, 30-60 frames (1-2 seconds at 30fps) is a good start.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated captions for the sampled frames.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}\")\n",
        "        return []\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {video_path}. Check file path and format.\")\n",
        "        return []\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Processing '{os.path.basename(video_path)}' (FPS: {fps}, Total Frames: {total_frames})\")\n",
        "\n",
        "    frame_captions = []\n",
        "    current_frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read() # Read a frame from the video\n",
        "\n",
        "        if not ret: # If no more frames, break the loop\n",
        "            break\n",
        "\n",
        "        # Process only selected frames based on frame_interval\n",
        "        if current_frame_count % frame_interval == 0:\n",
        "            try:\n",
        "                # OpenCV reads images in BGR format, VLM models usually expect RGB\n",
        "                pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "                # Generate caption for the current frame\n",
        "                # The pipeline returns a list of dictionaries, we want the 'generated_text'\n",
        "                caption_result = caption_pipeline(pil_image)\n",
        "                if caption_result and len(caption_result) > 0:\n",
        "                    generated_text = caption_result[0]['generated_text']\n",
        "                    frame_captions.append(f\"Frame {current_frame_count}: {generated_text}\")\n",
        "                    print(f\"  Generated caption for frame {current_frame_count}\")\n",
        "                else:\n",
        "                    print(f\"  No caption generated for frame {current_frame_count}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error captioning frame {current_frame_count}: {e}\")\n",
        "                # Continue processing even if one frame fails\n",
        "\n",
        "        current_frame_count += 1\n",
        "\n",
        "    cap.release() # Release the video capture object\n",
        "    print(f\"Finished processing frames for '{os.path.basename(video_path)}'.\")\n",
        "    return frame_captions\n",
        "\n",
        "def summarize_captions_with_llm(frame_captions):\n",
        "    \"\"\"\n",
        "    Uses an LLM (or a simpler approach for this challenge) to summarize\n",
        "    a list of individual frame captions into a more coherent video description.\n",
        "\n",
        "    For this challenge, we'll use a basic summarization method. For more advanced\n",
        "    summarization, you'd integrate another LLM (e.g., a summarization pipeline\n",
        "    from Hugging Face, or an external LLM API if you have access and keys).\n",
        "    \"\"\"\n",
        "    if not frame_captions:\n",
        "        return \"No visual content was detected or captioned from the video.\"\n",
        "\n",
        "    # Join a selection of captions to give a sense of the video's content.\n",
        "    # For a real LLM, you'd send these to a summarization model.\n",
        "    # Example using Hugging Face summarization pipeline (requires 'sentencepiece' and 'accelerate'):\n",
        "    # from transformers import pipeline\n",
        "    # summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "    # full_text = \"\\n\".join(frame_captions)\n",
        "    # summary_result = summarizer(full_text, max_length=150, min_length=30, do_sample=False)\n",
        "    # return summary_result[0]['summary_text']\n",
        "\n",
        "    # Simple concatenation for demonstration:\n",
        "    # Take a few captions from the beginning, middle, and end to form a general idea.\n",
        "    selected_captions = []\n",
        "    if len(frame_captions) > 0:\n",
        "        selected_captions.append(frame_captions[0]) # First frame\n",
        "    if len(frame_captions) > 2:\n",
        "        selected_captions.append(frame_captions[len(frame_captions) // 2]) # Middle frame\n",
        "    if len(frame_captions) > 1:\n",
        "        selected_captions.append(frame_captions[-1]) # Last frame\n",
        "\n",
        "    if len(selected_captions) < 3 and len(frame_captions) > len(selected_captions):\n",
        "        # If video is short, just include all.\n",
        "        return \"Video content: \" + \" \".join(frame_captions)\n",
        "\n",
        "    return \"Video content: \" + \" \".join(selected_captions) + \" (A more detailed summary would require an advanced LLM.)\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Enter the path of all the video, from your collab notebook which you want to label.\n",
        "    video_files = [\n",
        "        \"/content/ApplyEyeMakeup.avi\",\n",
        "        \"/content/HandStandPushups.avi\",\n",
        "        \"/content/PizzaTossing.avi\",\n",
        "        \"/content/SoccerPenalty.avi\",\n",
        "        \"/content/CuttingInKitchen.avi\",\n",
        "        \"/content/WritingOnBoard.avi\",\n",
        "        \"/content/Typing.avi\",\n",
        "        \"/content/WalkingWithDog.avi\",\n",
        "        \"/content/YoYo.avi\",\n",
        "        \"/content/UnevenBars (1).avi\"\n",
        "    ]\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    output_dir = \"video_descriptions\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize the VLM pipeline once\n",
        "    caption_pipeline = setup_captioning_pipeline()\n",
        "    if caption_pipeline is None:\n",
        "        print(\"Failed to initialize captioning pipeline. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    all_video_results = {}\n",
        "\n",
        "    for video_file_path in video_files:\n",
        "        print(f\"\\n--- Starting processing for: {os.path.basename(video_file_path)} ---\")\n",
        "\n",
        "        # Get frame-level captions\n",
        "        captions_for_current_video = process_video_for_captions(video_file_path, caption_pipeline, frame_interval=50)\n",
        "\n",
        "        # Summarize into a single description\n",
        "        full_video_description = summarize_captions_with_llm(captions_for_current_video)\n",
        "\n",
        "        print(f\"\\n--- Generated Description for {os.path.basename(video_file_path)} ---\")\n",
        "        print(full_video_description)\n",
        "\n",
        "        all_video_results[os.path.basename(video_file_path)] = full_video_description\n",
        "\n",
        "        # Save the description to a file\n",
        "        output_filename = os.path.join(output_dir, f\"{os.path.basename(video_file_path)}.txt\")\n",
        "        with open(output_filename, \"w\") as f:\n",
        "            f.write(full_video_description)\n",
        "        print(f\"Description saved to {output_filename}\")\n",
        "\n",
        "    print(\"\\n\\n--- All Video Descriptions Generated ---\")\n",
        "    for video_name, description in all_video_results.items():\n",
        "        print(f\"\\nVideo: {video_name}\")\n",
        "        print(f\"Description: {description}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded VLM model: Salesforce/blip-image-captioning-base\n",
            "\n",
            "--- Starting processing for: ApplyEyeMakeup.avi ---\n",
            "Processing 'ApplyEyeMakeup.avi' (FPS: 25.0, Total Frames: 152)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "Finished processing frames for 'ApplyEyeMakeup.avi'.\n",
            "\n",
            "--- Generated Description for ApplyEyeMakeup.avi ---\n",
            "Video content: Frame 0: a woman with long black hair Frame 100: a woman with long black hair and a pink lipstick Frame 150: a woman with long hair and a white shirt (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/ApplyEyeMakeup.avi.txt\n",
            "\n",
            "--- Starting processing for: HandStandPushups.avi ---\n",
            "Processing 'HandStandPushups.avi' (FPS: 25.0, Total Frames: 132)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "Finished processing frames for 'HandStandPushups.avi'.\n",
            "\n",
            "--- Generated Description for HandStandPushups.avi ---\n",
            "Video content: Frame 0: a group of people playing a game on a red carpet Frame 50: a man is dancing on the red carpet Frame 100: a man is doing a trick on a skateboard (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/HandStandPushups.avi.txt\n",
            "\n",
            "--- Starting processing for: PizzaTossing.avi ---\n",
            "Processing 'PizzaTossing.avi' (FPS: 25.0, Total Frames: 187)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "Finished processing frames for 'SoccerPenalty.avi'.\n",
            "\n",
            "--- Generated Description for SoccerPenalty.avi ---\n",
            "Video content: Frame 0: a soccer game on a tv screen Frame 50: a soccer game with a soccer field and a crowd Frame 100: a soccer game is shown on a tv screen (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/SoccerPenalty.avi.txt\n",
            "\n",
            "--- Starting processing for: CuttingInKitchen.avi ---\n",
            "Processing 'CuttingInKitchen.avi' (FPS: 25.0, Total Frames: 266)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "  Generated caption for frame 200\n",
            "  Generated caption for frame 250\n",
            "Finished processing frames for 'CuttingInKitchen.avi'.\n",
            "\n",
            "--- Generated Description for CuttingInKitchen.avi ---\n",
            "Video content: Frame 0: a person cutting a piece of paper with a knife Frame 150: a person cutting a piece of ice on a table Frame 250: a person cutting up some food on a cutting board (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/CuttingInKitchen.avi.txt\n",
            "\n",
            "--- Starting processing for: WritingOnBoard.avi ---\n",
            "Processing 'WritingOnBoard.avi' (FPS: 25.0, Total Frames: 191)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "Finished processing frames for 'WritingOnBoard.avi'.\n",
            "\n",
            "--- Generated Description for WritingOnBoard.avi ---\n",
            "Video content: Frame 0: a man standing in front of a blackboard Frame 100: a man is writing on a blackboard Frame 150: a man is writing on a blackboard (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/WritingOnBoard.avi.txt\n",
            "\n",
            "--- Starting processing for: Typing.avi ---\n",
            "Processing 'Typing.avi' (FPS: 25.0, Total Frames: 250)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "  Generated caption for frame 200\n",
            "Finished processing frames for 'Typing.avi'.\n",
            "\n",
            "--- Generated Description for Typing.avi ---\n",
            "Video content: Frame 0: a person using a keyboard to play a game Frame 100: a person is using a keyboard to play a game Frame 200: a person using a keyboard to play a game (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/Typing.avi.txt\n",
            "\n",
            "--- Starting processing for: WalkingWithDog.avi ---\n",
            "Processing 'WalkingWithDog.avi' (FPS: 29.97002997002997, Total Frames: 240)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "  Generated caption for frame 200\n",
            "Finished processing frames for 'WalkingWithDog.avi'.\n",
            "\n",
            "--- Generated Description for WalkingWithDog.avi ---\n",
            "Video content: Frame 0: a dog walking down a road in the woods Frame 100: a dog is walking down a dirt road Frame 200: a man walking down a road with a dog (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/WalkingWithDog.avi.txt\n",
            "\n",
            "--- Starting processing for: YoYo.avi ---\n",
            "Processing 'YoYo.avi' (FPS: 25.0, Total Frames: 187)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "Finished processing frames for 'YoYo.avi'.\n",
            "\n",
            "--- Generated Description for YoYo.avi ---\n",
            "Video content: Frame 0: a man holding a cell phone Frame 100: a man holding a bunch of green leaves Frame 150: a man holding a tree (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/YoYo.avi.txt\n",
            "\n",
            "--- Starting processing for: UnevenBars (1).avi ---\n",
            "Processing 'UnevenBars (1).avi' (FPS: 25.0, Total Frames: 194)\n",
            "  Generated caption for frame 0\n",
            "  Generated caption for frame 50\n",
            "  Generated caption for frame 100\n",
            "  Generated caption for frame 150\n",
            "Finished processing frames for 'UnevenBars (1).avi'.\n",
            "\n",
            "--- Generated Description for UnevenBars (1).avi ---\n",
            "Video content: Frame 0: a man is performing on a pole in a stadium Frame 100: a man is doing a trick on a pole Frame 150: a man is performing a trick on a stage (A more detailed summary would require an advanced LLM.)\n",
            "Description saved to video_descriptions/UnevenBars (1).avi.txt\n",
            "\n",
            "\n",
            "--- All Video Descriptions Generated ---\n",
            "\n",
            "Video: ApplyEyeMakeup.avi\n",
            "Description: Video content: Frame 0: a woman with long black hair Frame 100: a woman with long black hair and a pink lipstick Frame 150: a woman with long hair and a white shirt (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: HandStandPushups.avi\n",
            "Description: Video content: Frame 0: a group of people playing a game on a red carpet Frame 50: a man is dancing on the red carpet Frame 100: a man is doing a trick on a skateboard (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: PizzaTossing.avi\n",
            "Description: Video content: Frame 0: a man in a kitchen Frame 100: a man in a kitchen Frame 150: a man in a kitchen (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: SoccerPenalty.avi\n",
            "Description: Video content: Frame 0: a soccer game on a tv screen Frame 50: a soccer game with a soccer field and a crowd Frame 100: a soccer game is shown on a tv screen (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: CuttingInKitchen.avi\n",
            "Description: Video content: Frame 0: a person cutting a piece of paper with a knife Frame 150: a person cutting a piece of ice on a table Frame 250: a person cutting up some food on a cutting board (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: WritingOnBoard.avi\n",
            "Description: Video content: Frame 0: a man standing in front of a blackboard Frame 100: a man is writing on a blackboard Frame 150: a man is writing on a blackboard (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: Typing.avi\n",
            "Description: Video content: Frame 0: a person using a keyboard to play a game Frame 100: a person is using a keyboard to play a game Frame 200: a person using a keyboard to play a game (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: WalkingWithDog.avi\n",
            "Description: Video content: Frame 0: a dog walking down a road in the woods Frame 100: a dog is walking down a dirt road Frame 200: a man walking down a road with a dog (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: YoYo.avi\n",
            "Description: Video content: Frame 0: a man holding a cell phone Frame 100: a man holding a bunch of green leaves Frame 150: a man holding a tree (A more detailed summary would require an advanced LLM.)\n",
            "\n",
            "Video: UnevenBars (1).avi\n",
            "Description: Video content: Frame 0: a man is performing on a pole in a stadium Frame 100: a man is doing a trick on a pole Frame 150: a man is performing a trick on a stage (A more detailed summary would require an advanced LLM.)\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1DZxagrJW_U",
        "outputId": "a2a55f63-8139-49f8-cc4f-43c1d415e430"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0etoFIXVxHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}